'use client'

import { motion } from 'framer-motion'
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter'
import { oneLight } from 'react-syntax-highlighter/dist/esm/styles/prism'

export default function ChatGroqTutorial() {
  const scriptCode = `
  import os, random, requests, datetime
from dotenv import load_dotenv
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# ========================
# 1. Configuraci√≥n
# ========================
load_dotenv()
api_key = os.getenv("GROQ_API_KEY")
serper_api_key = os.getenv("SERPER_API_KEY")

if not api_key:
    raise ValueError("en el .env no hay una api valida de GROQ")

if not serper_api_key:
    raise ValueError("en el .env no hay una api valida de SERPER")

llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key=api_key,
    temperature=0.4,
)

# ========================
# 2. Prompt
# ========================
Prompt_estructura = """
[CONTEXTO]
Hoy es {fecha}.
Tienes acceso a memoria y a resultados de b√∫squeda web (cuando existan).
Responde de forma natural y conversacional:
- Si hay resultados de b√∫squeda: int√©gralos en tu respuesta sin sonar forzado.
- Si no hay resultados de b√∫squeda: responde con tu conocimiento previo, pero menciona brevemente que puede estar desactualizado.
- Siempre prioriza ser √∫til, claro y conciso, evitando repetir advertencias innecesarias.

[BUSQUEDA WEB]
{busqueda}

[MEMORIA]
{historial}

[ENTRADA DEL USUARIO]
Consulta: {mensaje}

[RESPUESTA COMO {rol}]
"""

prompt = PromptTemplate(
    input_variables=["rol", "mensaje", "historial", "busqueda", "fecha"],
    template=Prompt_estructura.strip(),
)

# ========================
# 3. Estado global
# ========================
class State(TypedDict):
    mensaje: str
    rol: str
    historial: str
    respuesta: str
    busqueda: str
    user_id: str

usuarios = {}

def get_memory(user_id: str):
    if user_id not in usuarios:
        usuarios[user_id] = ConversationBufferMemory(
            memory_key="historial", input_key="mensaje"
        )
    return usuarios[user_id]

# ========================
# 4. Nodo de b√∫squeda Serper
# ========================
def serper_node(state: State) -> State:
    try:
        q = state.get("mensaje", "")
        headers = {"X-API-KEY": serper_api_key, "Content-Type": "application/json"}
        resp = requests.post(
            "https://google.serper.dev/search",
            headers=headers,
            json={"q": q},
        )
        data = resp.json()
        if "organic" in data and len(data["organic"]) > 0:
            resumen = [
                f"{item.get('title')} - {item.get('link','')}"
                for item in data["organic"][:3]
            ]
            state["busqueda"] = " | ".join(resumen)
        else:
            state["busqueda"] = "No hubo resultados"
    except Exception as e:
        state["busqueda"] = f"Error en b√∫squeda: {e}"
    
    print("DEBUG b√∫squeda:", state["busqueda"])  # üëà Debug
    return state

# ========================
# 5. Nodo agente (usa b√∫squeda + historial)
# ========================
def agente_node(state: State) -> State:
    memory = get_memory(state.get("user_id", "default"))
    historial = memory.load_memory_variables({}).get("historial", "")
    fecha = datetime.date.today().strftime("%d/%m/%Y")

    # üîë L√≥gica: decidir si activar b√∫squeda
    activar_busqueda = any(
        palabra in state["mensaje"].lower()
        for palabra in ["qui√©n", "cu√°ndo", "actual", "√∫ltimo", "presidente", "hoy", "noticia", "√∫ltima hora"]
    )

    if activar_busqueda:
        state = serper_node(state)  # ejecuta b√∫squeda solo si hace falta

  

    texto_prompt = prompt.format(
        rol=state["rol"],
        mensaje=state["mensaje"],
        historial=historial,
        busqueda=state.get("busqueda", ""),
        fecha=fecha,
    )
    respuesta = llm.invoke(texto_prompt).content

    # Guardar en memoria
    memory.save_context({"mensaje": state["mensaje"]}, {"respuesta": respuesta})

    state["respuesta"] = respuesta
    state["historial"] = historial
    return state

# ========================
# 6. Grafo
# ========================
workflow = StateGraph(State)
workflow.add_node("serper", serper_node)
workflow.add_node("agente", agente_node)

workflow.set_entry_point("agente")   # üëà ahora empieza en el agente
workflow.add_edge("agente", END)

app = workflow.compile()

# ========================
# 7. CLI
# ========================
print("LLM iniciado con LangGraph + Serper din√°mico")
user_id = str(random.randint(10000, 90000))
rol = "auditor"
print(f"tu user id es {user_id}")

while True:
    user_input = input("Tu: ")
    if user_input.lower() == "salir":
        break

    result = app.invoke(
        {"mensaje": user_input, "rol": rol, "historial": "", "user_id": user_id}
    )
    print("LLM:", result["respuesta"])

  
  `

  const explanationJSX = (
    <>
      <p>
        La verdadera potencia de este script no est√° √∫nicamente en el LLM <strong>ChatGroq</strong>, sino en c√≥mo <strong>la arquitectura modular y los nodos gestionan la informaci√≥n, la memoria y las b√∫squedas web</strong> para crear flujos inteligentes y escalables. Cada componente est√° dise√±ado para manejar entradas, ejecutar el modelo y mantener coherencia, permitiendo una IA que no solo responde, sino que aprende y se adapta.
      </p>
      <ol className="list-decimal pl-6 mt-4 space-y-3 text-sm">
        <li>
          <strong>Nodos como bloques de construcci√≥n:</strong> Cada nodo representa una unidad de l√≥gica independiente. Por ejemplo, <strong>el nodo de Serper</strong> se encarga de consultar informaci√≥n actualizada en internet antes de generar la respuesta. Otros nodos pueden formatear prompts, invocar el LLM o guardar la memoria. Esto permite que los flujos sean visuales, testeables y f√°cilmente ampliables.
        </li>
        <li>
          <strong>Nodo de b√∫squeda Serper:</strong> Este nodo hace una petici√≥n HTTP a <strong>Serper API</strong> con la consulta del usuario, recupera los resultados m√°s relevantes y los integra directamente en el prompt. As√≠, la IA combina conocimiento previo con informaci√≥n actual, ofreciendo respuestas precisas, actualizadas y contextuales.
        </li>
        <li>
          <strong>Acceso a memoria integrada:</strong> Gracias a <strong>ConversationBufferMemory</strong>, cada usuario mantiene un historial de interacciones. Cuando un nodo necesita generar una respuesta, puede consultar la memoria para contextualizar la respuesta y asegurar coherencia en toda la conversaci√≥n. Esto es fundamental para mantener un di√°logo continuo y personalizado.
        </li>
        <li>
          <strong>Estado global y control de flujo:</strong> La estructura con <strong>TypedDict State</strong> centraliza la informaci√≥n de cada usuario: mensaje, rol, historial, b√∫squeda y resultado. Cada nodo puede leer y actualizar este estado, asegurando que el flujo no pierda contexto y que cada respuesta se construya de manera consistente. LangGraph permite definir claramente la secuencia de nodos y su interconexi√≥n.
        </li>
        <li>
          <strong>Integraci√≥n con prompts din√°micos:</strong> El prompt se construye combinando historial, b√∫squeda web y mensaje actual, usando <strong>PromptTemplate</strong>. Esto asegura que la IA interprete correctamente la intenci√≥n del usuario y genere respuestas coherentes, naturales y enriquecidas con informaci√≥n en tiempo real.
        </li>
        <li>
          <strong>Escalabilidad y modularidad:</strong> Gracias a esta organizaci√≥n, se pueden agregar nuevos nodos para manejar distintos roles, an√°lisis de datos adicionales o integraciones externas. Cada nodo mantiene su l√≥gica independiente, mientras que la memoria global y la integraci√≥n web aseguran coherencia y continuidad en todo el flujo.
        </li>
        <li>
          <strong>Visi√≥n pr√°ctica y empresarial:</strong> Con esta estructura, la IA deja de ser un ‚Äúcaj√≥n negro‚Äù: los procesos son auditables, replicables y visualmente trazables. La combinaci√≥n de nodos, memoria y acceso web permite crear diagn√≥sticos progresivos, recomendaciones acumulativas y una experiencia de usuario adaptable.
        </li>
      </ol>
      <p className="mt-3">
        <strong>En resumen:</strong> Este script transforma al LLM en un sistema inteligente y modular: los nodos encapsulan procesos, la memoria mantiene continuidad, y la integraci√≥n con internet a trav√©s del <strong>nodo Serper</strong> permite generar respuestas precisas y actualizadas. Cada flujo se convierte en un bloque de arquitectura escalable listo para integrarse en sistemas m√°s complejos y automatizados.
      </p>
    </>
  );
  
  
  

  return (
    <div className="w-full flex flex-col md:flex-row gap-6">
      {/* Contenedor C√≥digo */}
      <div className="flex-1 bg-white border border-gray-300 rounded-xl p-4 max-h-[500px] overflow-auto shadow-md">
        <h3 className="text-lg font-bold mb-2 text-orange-500">C√≥digo de ejemplo</h3>
        <SyntaxHighlighter
          language="python"
          style={oneLight}
          wrapLines
          showLineNumbers
          customStyle={{ fontSize: '0.875rem', backgroundColor: '#ffffff' }}
          lineProps={{ style: { wordBreak: 'break-word', whiteSpace: 'pre-wrap' } }}
          codeTagProps={{
            style: { fontFamily: 'Fira Code, monospace', color: '#000' },
          }}
        >
          {scriptCode}
        </SyntaxHighlighter>
      </div>

      {/* Contenedor Explicaci√≥n */}
      <motion.div
        className="flex-1 bg-white border border-gray-300 rounded-xl p-4 max-h-[500px] overflow-auto shadow-md"
        initial={{ opacity: 0, y: 8 }}
        animate={{ opacity: 1, y: 0 }}
        transition={{ duration: 0.35 }}
      >
        <h3 className="text-lg font-bold mb-2 text-orange-500">Explicaci√≥n detallada</h3>
        <div className="text-gray-800 leading-relaxed text-sm">{explanationJSX}</div>
      </motion.div>
    </div>
  )
}
